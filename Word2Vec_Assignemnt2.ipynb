{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8116f34e-b84d-4be0-b5c1-a898c4b7f10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd536a7a-e800-4fa4-81df-0332f7b50afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_pretrained = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfac32cb-65ea-4fce-a508-fea8133a1360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071),\n",
       " ('monarch', 0.6189674735069275),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321243286133),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.5181134343147278),\n",
       " ('sultan', 0.5098593235015869),\n",
       " ('monarchy', 0.5087411403656006)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_pretrained.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14300c5d-cfc5-42a7-aeef-a55f84939499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Nepal', 0.619372546672821),\n",
       " ('Delhi_Oct.##_ANI', 0.6125845909118652),\n",
       " ('Delhi_Mar.##', 0.5995363593101501),\n",
       " ('Indias', 0.5982224345207214),\n",
       " ('Himachal_Pradesh', 0.5855835676193237),\n",
       " ('Delhi_Jan.##_ANI', 0.5768283605575562),\n",
       " ('Indiaâ_€_™', 0.5768230557441711),\n",
       " ('Delhi_Nov.##_ANI', 0.5760971903800964),\n",
       " ('Delhi_Aug.##_ANI', 0.575374186038971),\n",
       " ('NEW_DELHI', 0.5662563443183899)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_pretrained.most_similar(positive=[\"India\", \"Delhi\"], negative=[\"Mumbai\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbf9e6b8-42f7-479e-bf1e-bf0ee0d62752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('coconut', 0.6269199252128601),\n",
       " ('bananas', 0.6094670295715332),\n",
       " ('pineapple', 0.5931852459907532),\n",
       " ('mangoes', 0.581656277179718),\n",
       " ('cashew', 0.5697671175003052),\n",
       " ('papaya', 0.5613257884979248),\n",
       " ('cashew_nuts', 0.5515928864479065),\n",
       " ('pineapples', 0.5468831658363342),\n",
       " ('mangos', 0.5464771389961243),\n",
       " ('cashew_nut', 0.5408104658126831)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_pretrained.most_similar(positive=[\"mango\", \"banana\"], negative=[\"apple\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b27b6d5-8113-40d5-af2d-9bf25d0d8d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - man + woman ~= queen\n",
      "India - Mumbai + Delhi ~= Nepal\n",
      "banana - apple + mango ~= coconut\n"
     ]
    }
   ],
   "source": [
    "# Find most similar words\n",
    "word_pairs = [\n",
    "    (\"king\", \"man\", \"woman\"),       \n",
    "    (\"India\", \"Mumbai\", \"Delhi\"), \n",
    "    (\"banana\", \"apple\", \"mango\"),      \n",
    "]\n",
    "\n",
    "for words in word_pairs:\n",
    "    similar_words = wv_pretrained.most_similar(positive=[words[0], words[2]], negative=[words[1]])\n",
    "    print(f\"{words[0]} - {words[1]} + {words[2]} ~= {similar_words[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01de7b00-5fc8-4f0b-a372-c9b8b1f5c6be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6890953d-e13a-476c-8468-e8a0730b3422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0554164f-e3fd-45aa-b33a-6a30747cd91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = pd.read_csv(\"IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdbf4853-3fb6-4cdc-9a6f-e5ec045803da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf13be03-e246-4d4e-9b9e-e7c55d0a3226",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7f7fe23-e8d4-4d1c-b0af-9c2f3d094102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        return ' '.join(filtered_words)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "imdb['review'] = imdb['review'].apply(remove_stopwords)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75bb4961-535e-4b77-b8ea-04a932586a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One reviewers mentioned watching 1 Oz episode ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production br br filming tech...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically theres family little boy Jake thinks...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Matteis Love Time Money visually stunni...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>thought movie right good job wasnt creative or...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot bad dialogue bad acting idiotic direc...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>Catholic taught parochial elementary schools n...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Im going disagree previous comment side Maltin...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>one expects Star Trek movies high art fans exp...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One reviewers mentioned watching 1 Oz episode ...  positive\n",
       "1      wonderful little production br br filming tech...  positive\n",
       "2      thought wonderful way spend time hot summer we...  positive\n",
       "3      Basically theres family little boy Jake thinks...  negative\n",
       "4      Petter Matteis Love Time Money visually stunni...  positive\n",
       "...                                                  ...       ...\n",
       "49995  thought movie right good job wasnt creative or...  positive\n",
       "49996  Bad plot bad dialogue bad acting idiotic direc...  negative\n",
       "49997  Catholic taught parochial elementary schools n...  negative\n",
       "49998  Im going disagree previous comment side Maltin...  negative\n",
       "49999  one expects Star Trek movies high art fans exp...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8957308d-745e-4b6b-b16a-84b8ee68f1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = imdb['review'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "618311b8-9219-47c4-92db-05cdda992c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram = Word2Vec(\n",
    "    sentences= tokenized_data,\n",
    "    sg=1,  \n",
    "    vector_size=50,  \n",
    "    window=5,  \n",
    "    min_count=5,  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf3343d0-c76b-4164-8624-3a83a8d43418",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = Word2Vec(\n",
    "    sentences=tokenized_data,\n",
    "    sg=0,  \n",
    "    vector_size=50,  \n",
    "    window=5,  \n",
    "    min_count=5,  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ab66ee1-efcc-48e5-a7fd-ce12b3867630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_vector(sentence, model):\n",
    "    words = sentence.split()\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "imdb['skipgram_vector'] = imdb['review'].apply(lambda x: get_average_vector(x, skipgram))\n",
    "imdb['cbow_vector'] = imdb['review'].apply(lambda x: get_average_vector(x, cbow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f8fe1fc-99b9-4108-845d-73ef1d2a094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(x_train, x_test, y_train, y_test):\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    return classification_report(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98c553f8-2df6-4353-854e-ef7e62ed5aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "skip = np.array(imdb['skipgram_vector'].tolist())\n",
    "cbow = np.array(imdb['cbow_vector'].tolist())\n",
    "y = imdb['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "sx_train, sx_test, y_train, y_test = train_test_split(skip, y, test_size=0.2, random_state=42)\n",
    "cx_train, cx_test, _, _ = train_test_split(cbow, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6edf4c6c-f4c7-4a0d-88af-418a513378a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-gram Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.87      4961\n",
      "           1       0.87      0.87      0.87      5039\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Skip-gram Model:\")\n",
    "print(regression(sx_train, sx_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30a6be1e-3fda-418a-97b2-31360e1eca5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBoW Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.84      0.85      4961\n",
      "           1       0.84      0.86      0.85      5039\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"CBoW Model:\")\n",
    "print(regression(cx_train, cx_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b448c277-6780-428e-ae6a-9151289735b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_path = 'GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db643167-62dc-48b4-98cd-8ce945ce5278",
   "metadata": {},
   "source": [
    "Pretrained Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aded0ba5-44e1-41bb-85dc-0815b5c48a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained Word2Vec Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85      4961\n",
      "           1       0.85      0.86      0.85      5039\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.85      0.85      0.85     10000\n",
      "weighted avg       0.85      0.85      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_average_vector(sentence, model):\n",
    "    words = sentence.split()\n",
    "    vectors = [model[word] for word in words if word in model]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "imdb['new_vector'] = imdb['review'].apply(lambda x: get_average_vector(x, model))\n",
    "\n",
    "\n",
    "new_x = np.array(imdb['new_vector'].tolist())\n",
    "y = imdb['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "\n",
    "new_x_train, new_x_test, y_train, y_test = train_test_split(new_x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Pretrained Word2Vec Model:\")\n",
    "print(regression(new_x_train, new_x_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfad395-81a1-4ec3-9468-ba9be024501d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
