{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715},{"sourceId":9302948,"sourceType":"datasetVersion","datasetId":5633087}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-02T16:22:54.006841Z","iopub.execute_input":"2024-09-02T16:22:54.007236Z","iopub.status.idle":"2024-09-02T16:22:54.022908Z","shell.execute_reply.started":"2024-09-02T16:22:54.007200Z","shell.execute_reply":"2024-09-02T16:22:54.022018Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report\nfrom nltk.tokenize import word_tokenize\nimport pandas as pd\nimport re\nimport nltk\nnltk.download('punkt')\n\ndf = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\nreviews = df['review'].values\nsentiments = df['sentiment'].values\n\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'[^a-z\\s]', '', text)\n    return word_tokenize(text)\n\ntokenized_reviews = [preprocess_text(review) for review in reviews]\n\ndef load_glove_embeddings(glove_path):\n    embeddings_index = {}\n    with open(glove_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            coeffs = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = coeffs\n    return embeddings_index\n\nglove_path = '/kaggle/input/glove2/glove.6B.100d.txt'\nembeddings_index = load_glove_embeddings(glove_path)\n\nvocab = set(word for review in tokenized_reviews for word in review)\nword_to_index = {word: i+1 for i, word in enumerate(vocab, 1)}\nword_to_index[''] = 0\nvocab_size = len(word_to_index) + 1\n\nembedding_dim = 100\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, i in word_to_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\nmax_len = 100\ndef text_to_sequence(text, word_to_index, max_len):\n    return [word_to_index.get(word, 0) for word in text][:max_len] + [0] * max(0, max_len - len(text))\n\nsequences = [text_to_sequence(review, word_to_index, max_len) for review in tokenized_reviews]\n\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(sentiments)\n\nX_train, X_test, y_train, y_test = train_test_split(sequences, y, test_size=0.2, random_state=42)\n\ndef create_data_loader(X, y, batch_size):\n    tensor_X = torch.tensor(X, dtype=torch.long)\n    tensor_y = torch.tensor(y, dtype=torch.float32)\n    dataset = TensorDataset(tensor_X, tensor_y)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\ntrain_loader = create_data_loader(X_train, y_train, batch_size=128)\ntest_loader = create_data_loader(X_test, y_test, batch_size=128)\n\n# RNN Model\nclass GloveRNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, embedding_matrix):\n        super(GloveRNN, self).__init__()\n        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        h, _ = self.rnn(x)\n        h = h[:, -1, :]\n        return self.fc(h)\n\nhidden_dim = 64\nmodel = GloveRNN(vocab_size, embedding_dim, hidden_dim, embedding_matrix)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\ndef train_model(model, train_loader, epochs):\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        for inputs, targets in train_loader:\n            optimizer.zero_grad()\n            outputs = model(inputs).squeeze()\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}')\n\ndef evaluate_model(model, test_loader):\n    model.eval()\n    y_true = []\n    y_pred = []\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            outputs = model(inputs).squeeze()\n            preds = torch.sigmoid(outputs).round().long()\n            y_true.extend(targets.tolist())\n            y_pred.extend(preds.tolist())\n    return classification_report(y_true, y_pred, target_names=label_encoder.classes_)\n\ntrain_model(model, train_loader, epochs=5)\nprint(\"Classification Report for GloVe + RNN:\")\nprint(evaluate_model(model, test_loader))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T16:49:57.250138Z","iopub.execute_input":"2024-09-02T16:49:57.250514Z","iopub.status.idle":"2024-09-02T16:53:01.006181Z","shell.execute_reply.started":"2024-09-02T16:49:57.250480Z","shell.execute_reply":"2024-09-02T16:53:01.005340Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nEpoch 1, Loss: 0.6868285711961813\nEpoch 2, Loss: 0.6908077214853451\nEpoch 3, Loss: 0.6898445776476266\nEpoch 4, Loss: 0.6856517462303844\nEpoch 5, Loss: 0.6858390838193437\nClassification Report for GloVe + RNN:\n              precision    recall  f1-score   support\n\n    negative       0.54      0.54      0.54      4961\n    positive       0.54      0.54      0.54      5039\n\n    accuracy                           0.54     10000\n   macro avg       0.54      0.54      0.54     10000\nweighted avg       0.54      0.54      0.54     10000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#LSTM Model\nclass GloveLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, embedding_matrix):\n        super(GloveLSTM, self).__init__()\n        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=0.5)\n        self.fc = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        h, _ = self.lstm(x)\n        h = h[:, -1, :]\n        return self.fc(h)\n\nhidden_dim = 64\nmodel = GloveLSTM(vocab_size, embedding_dim, hidden_dim, num_layers=1, embedding_matrix=embedding_matrix)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\ntrain_model(model, train_loader, epochs=5)\nprint(\"Classification Report for GloVe + LSTM:\")\nprint(evaluate_model(model, test_loader))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-02T16:53:55.761270Z","iopub.execute_input":"2024-09-02T16:53:55.762118Z","iopub.status.idle":"2024-09-02T16:55:22.474814Z","shell.execute_reply.started":"2024-09-02T16:53:55.762074Z","shell.execute_reply":"2024-09-02T16:55:22.473799Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n  warnings.warn(\"dropout option adds dropout after all but last \"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 0.6535816914357316\nEpoch 2, Loss: 0.603064070399196\nEpoch 3, Loss: 0.5707171725960204\nEpoch 4, Loss: 0.5125554716244293\nEpoch 5, Loss: 0.49410779026750556\nClassification Report for GloVe + LSTM:\n              precision    recall  f1-score   support\n\n    negative       0.81      0.71      0.76      4961\n    positive       0.74      0.84      0.79      5039\n\n    accuracy                           0.77     10000\n   macro avg       0.78      0.77      0.77     10000\nweighted avg       0.78      0.77      0.77     10000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#OnTheFly RNN\nclass OnTheFlyRNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n        super(OnTheFlyRNN, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        h, _ = self.rnn(x)\n        h = h[:, -1, :]\n        return self.fc(h)\n\nmodel = OnTheFlyRNN(vocab_size, embedding_dim, hidden_dim)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Train and evaluate\ntrain_model(model, train_loader, epochs=5)\nprint(\"Classification Report for On-the-Fly Embeddings + RNN:\")\nprint(evaluate_model(model, test_loader))","metadata":{"execution":{"iopub.status.busy":"2024-09-02T16:56:03.716224Z","iopub.execute_input":"2024-09-02T16:56:03.717222Z","iopub.status.idle":"2024-09-02T17:01:27.211641Z","shell.execute_reply.started":"2024-09-02T16:56:03.717171Z","shell.execute_reply":"2024-09-02T17:01:27.210461Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.6934552581165545\nEpoch 2, Loss: 0.6783685343334088\nEpoch 3, Loss: 0.6701521563072936\nEpoch 4, Loss: 0.6560366377472496\nEpoch 5, Loss: 0.6085136558491582\nClassification Report for On-the-Fly Embeddings + RNN:\n              precision    recall  f1-score   support\n\n    negative       0.68      0.65      0.66      4961\n    positive       0.67      0.70      0.68      5039\n\n    accuracy                           0.67     10000\n   macro avg       0.67      0.67      0.67     10000\nweighted avg       0.67      0.67      0.67     10000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#OnTheFly LSTM\nclass OnTheFlyLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n        super(OnTheFlyLSTM, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=0.5)\n        self.fc = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        h, _ = self.lstm(x)\n        h = h[:, -1, :]\n        return self.fc(h)\n\nmodel = OnTheFlyLSTM(vocab_size, embedding_dim, hidden_dim)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Train and evaluate\ntrain_model(model, train_loader, epochs=5)\nprint(\"Classification Report for On-the-Fly Embeddings + LSTM:\")\nprint(evaluate_model(model, test_loader))","metadata":{"execution":{"iopub.status.busy":"2024-09-02T17:02:09.557610Z","iopub.execute_input":"2024-09-02T17:02:09.557983Z","iopub.status.idle":"2024-09-02T17:07:29.095442Z","shell.execute_reply.started":"2024-09-02T17:02:09.557935Z","shell.execute_reply":"2024-09-02T17:07:29.094171Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n  warnings.warn(\"dropout option adds dropout after all but last \"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 0.6765606542364858\nEpoch 2, Loss: 0.5795277995042527\nEpoch 3, Loss: 0.45404979215262414\nEpoch 4, Loss: 0.38108627041117454\nEpoch 5, Loss: 0.28759989880334835\nClassification Report for On-the-Fly Embeddings + LSTM:\n              precision    recall  f1-score   support\n\n    negative       0.79      0.86      0.82      4961\n    positive       0.85      0.77      0.81      5039\n\n    accuracy                           0.81     10000\n   macro avg       0.82      0.81      0.81     10000\nweighted avg       0.82      0.81      0.81     10000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}